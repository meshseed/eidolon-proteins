id: 88393c5b-ab7f-4a87-831d-bcd5781b6661
title: Unified LLM Provider with Local & API Support
summary: >-
  The system now supports both local LLM execution (via Ollama, Gemma) and API-based models (like
  Gemini), with a unified provider abstraction. This enables flexible deployment for both developers
  and end-users, while clearly delineating compatibility for embeddings.
insights:
  - >-
    A provider abstraction layer unifies local and API-based LLM calls, simplifying integration and
    user configuration.
  - >-
    Local LLM support (e.g., Gemma 2B via Ollama) is now configurable in the PWA settings, requiring
    users to run Ollama and pull models separately.
  - >-
    Embeddings generated by local models are not compatible with Gemini, a crucial compatibility
    warning for users.
  - >-
    The UI now guides home users through setting up local LLMs, removing hardcoded API key
    dependencies for core functionalities like ingestion and querying.
  - >-
    Documentation has been updated with Windows-specific steps for local LLM setup, catering to a
    broader user base.
  - >-
    The system ensures that core features like regeneration and chat function correctly regardless
    of the chosen LLM provider (local or API).
tags:
  - '#llm'
  - '#provider'
  - '#local'
  - '#api'
  - '#ollama'
  - '#gemma'
  - '#gemini'
  - '#pwa'
  - '#configuration'
  - '#public'
  - '#embed:gemini'
  - '#embed:nomic-v1.5'
  - '#dna:Model_Selection___UI_Polish_md'
  - '#synthesis:v4.5'
tier: reference
coherence_score: 0.97
created_at: '2026-02-07T02:29:57.087Z'
source: synthesis
emotional_gradient: curiosity → insight → understanding
